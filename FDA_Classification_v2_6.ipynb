{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Link:\n",
        "```\n",
        "https://www.fda.gov/drugs/drug-approvals-and-databases/ndc-product-file-definitions\n",
        "```"
      ],
      "metadata": {
        "id": "l2rdqNN82V83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing all the required libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "mCzK-2hyA7oS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kyKssge4-i6E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import gc\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "from wordcloud import WordCloud\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, Embedding, Flatten, Concatenate, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import joblib\n",
        "import shap\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Creating folders to save outputs and models\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Setting the memory usage less space\n",
        "plt.rcParams['figure.max_open_warning'] = 50\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading in a Otimized way\n",
        "\n"
      ],
      "metadata": {
        "id": "ItKxgkjbBMOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load the FDA drug product data with memory optimization\n",
        "    \"\"\"\n",
        "    print(f\"Loading data from {file_path}...\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Load only necessary columns\n",
        "        df = pd.read_excel(\n",
        "            file_path,\n",
        "            usecols=['PROPRIETARYNAME', 'NONPROPRIETARYNAME', 'ROUTENAME',\n",
        "                     'DOSAGEFORMNAME', 'SUBSTANCENAME', 'PHARM_CLASSES',\n",
        "                     'LABELERNAME', 'MARKETINGCATEGORYNAME',\n",
        "                     'STARTMARKETINGDATE']\n",
        "        )\n",
        "        print(f\"Data loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
        "        print(f\"Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "zAJu27Sd-6q-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning and Preprocessing"
      ],
      "metadata": {
        "id": "I-h0FijnBcU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_preprocess_data(df):\n",
        "    \"\"\"\n",
        "    Clean and preprocess the FDA drug product data\n",
        "    \"\"\"\n",
        "    print(\"Starting data cleaning and preprocessing...\")\n",
        "\n",
        "    # Ploting original distribution of MARKETINGCATEGORYNAME\n",
        "    original_distribution = df['MARKETINGCATEGORYNAME'].value_counts()\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    ax = original_distribution.plot(kind='bar', color=sns.color_palette(\"viridis\", len(original_distribution)))\n",
        "    plt.title('Original Marketing Category Distribution')\n",
        "    plt.xlabel('Marketing Category')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=90)\n",
        "\n",
        "    # Adding count labels on top of bars\n",
        "    for i, v in enumerate(original_distribution.values):\n",
        "        ax.text(i, v + 5, str(v), ha='center')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/original_category_distribution.jpg')\n",
        "    plt.close()\n",
        "\n",
        "    # Group MARKETINGCATEGORYNAME into consolidated labels\n",
        "    marketing_category_mapping = {\n",
        "        'OTC MONOGRAPH DRUG': 'OTC',\n",
        "        'OTC MONOGRAPH FINAL': 'OTC',\n",
        "        'OTC MONOGRAPH NOT FINAL': 'OTC',\n",
        "        'UNAPPROVED DRUG FOR USE IN DRUG SHORTAGE': 'UNAPPROVED',\n",
        "        'UNAPPROVED MEDICAL GAS': 'UNAPPROVED',\n",
        "        'UNAPPROVED DRUG OTHER': 'UNAPPROVED',\n",
        "        'UNAPPROVED HOMEOPATHIC': 'UNAPPROVED',\n",
        "        'NDA': 'NDA',\n",
        "        'NDA AUTHORIZED GENERIC': 'NDA',\n",
        "        'ANDA': 'ANDA',\n",
        "        'BLA': 'BLA'\n",
        "    }\n",
        "\n",
        "    # Creating a new column with grouped categories\n",
        "    df['GROUPED_MARKETING_CATEGORY'] = df['MARKETINGCATEGORYNAME'].map(\n",
        "        marketing_category_mapping, na_action='ignore')\n",
        "\n",
        "    # Drop rows with EMERGENCY USE AUTHORIZATION and Null values in grouped target\n",
        "    df = df[df['MARKETINGCATEGORYNAME'] != 'EMERGENCY USE AUTHORIZATION']\n",
        "    df = df.dropna(subset=['GROUPED_MARKETING_CATEGORY'])\n",
        "\n",
        "    # Fill textual nulls with 'no text' to avoid the null and affect the performance\n",
        "    text_columns = [\n",
        "        'PROPRIETARYNAME', 'NONPROPRIETARYNAME', 'ROUTENAME',\n",
        "        'DOSAGEFORMNAME', 'SUBSTANCENAME', 'PHARM_CLASSES',\n",
        "        'LABELERNAME'\n",
        "    ]\n",
        "    for col in text_columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna('no text')\n",
        "\n",
        "    # Convert STARTMARKETINGDATE to datetime if present to use in EDA\n",
        "    if 'STARTMARKETINGDATE' in df.columns:\n",
        "        df['STARTMARKETINGDATE'] = pd.to_datetime(df['STARTMARKETINGDATE'], errors='coerce')\n",
        "        df['MARKETING_YEAR'] = df['STARTMARKETINGDATE'].dt.year\n",
        "\n",
        "    # Ploting grouped distribution\n",
        "    grouped_distribution = df['GROUPED_MARKETING_CATEGORY'].value_counts()\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ax = grouped_distribution.plot(kind='bar', color=sns.color_palette(\"viridis\", len(grouped_distribution)))\n",
        "    plt.title('Grouped Marketing Category Distribution')\n",
        "    plt.xlabel('Marketing Category')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    # To add count labels on top of bars\n",
        "    for i, v in enumerate(grouped_distribution.values):\n",
        "        ax.text(i, v + 5, str(v), ha='center')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/grouped_category_distribution.jpg')\n",
        "    plt.close()\n",
        "\n",
        "    # Creating a missing value heatmap\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
        "    plt.title('Missing Value Heatmap')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/missing_value_heatmap.jpg')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot Year wise listing distribution\n",
        "    if 'MARKETING_YEAR' in df.columns:\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        year_counts = df['MARKETING_YEAR'].value_counts().sort_index()\n",
        "        year_counts.plot(kind='line', marker='o')\n",
        "        plt.title('Year-wise Drug Listing Distribution')\n",
        "        plt.xlabel('Year')\n",
        "        plt.ylabel('Number of Listings')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('outputs/year_wise_distribution.jpg')\n",
        "        plt.close()\n",
        "\n",
        "    df = df.reset_index(drop=True)\n",
        "    print(f\"Data cleaned and preprocessed. Remaining rows: {df.shape[0]}\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "LucqACsjRTps"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "4MYmCURRCi5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def engineer_features(df):\n",
        "    \"\"\"\n",
        "    Engineer features for modeling with memory optimization\n",
        "    \"\"\"\n",
        "    print(\"Starting feature engineering...\")\n",
        "\n",
        "    df['GROUPED_MARKETING_CATEGORY'] = df['GROUPED_MARKETING_CATEGORY'].astype('category')\n",
        "\n",
        "    # Text length and word count features\n",
        "    for col in ['PROPRIETARYNAME', 'DOSAGEFORMNAME', 'SUBSTANCENAME', 'NONPROPRIETARYNAME']:\n",
        "        if col in df.columns:\n",
        "            df[f'{col}_LENGTH'] = df[col].str.len()\n",
        "            df[f'{col}_WORD_COUNT'] = df[col].str.split().str.len()\n",
        "\n",
        "    # Activing Ingredient Count\n",
        "    if 'SUBSTANCENAME' in df.columns:\n",
        "        df['ACTIVE_INGREDIENT_COUNT'] = df['SUBSTANCENAME'].str.count(';') + 1\n",
        "        df.loc[df['SUBSTANCENAME'] == 'no text', 'ACTIVE_INGREDIENT_COUNT'] = 0\n",
        "\n",
        "    # Pharmaceutical Class Count (imputed with -1 due to overfitting and deviating the features from actual learning, earlier tried with 0)\n",
        "    if 'PHARM_CLASSES' in df.columns:\n",
        "        df['PHARM_CLASS_COUNT'] = df['PHARM_CLASSES'].str.count(';') + 1\n",
        "        df.loc[df['PHARM_CLASSES'] == 'no text', 'PHARM_CLASS_COUNT'] = -1\n",
        "\n",
        "        key_classes = ['analgesic', 'antibiotic', 'antiviral', 'cardiovascular', 'central nervous system',\n",
        "                       'anti-inflammatory', 'hormone', 'vaccine', 'immunologic']\n",
        "        for cls in key_classes:\n",
        "            df[f'IS_{cls.upper().replace(\" \", \"_\")}'] = df['PHARM_CLASSES'].str.contains(cls, case=False, na=False).astype(np.int8)\n",
        "\n",
        "    # TF-IDF for SUBSTANCENAME\n",
        "    if 'SUBSTANCENAME' in df.columns:\n",
        "        print(\"Generating TF-IDF features for SUBSTANCENAME...\")\n",
        "        tfidf = TfidfVectorizer(max_features=50, stop_words='english')\n",
        "        substance_tfidf = tfidf.fit_transform(df['SUBSTANCENAME'].fillna('no text'))\n",
        "        tfidf_feature_names = [f'SUBSTANCE_TFIDF_{i}' for i in range(substance_tfidf.shape[1])]\n",
        "        substance_tfidf_df = pd.DataFrame(substance_tfidf.toarray(), columns=tfidf_feature_names)\n",
        "        df = pd.concat([df, substance_tfidf_df], axis=1)\n",
        "        print(\"Top TF-IDF terms:\")\n",
        "        feature_names = tfidf.get_feature_names_out()\n",
        "        for i, feature_name in enumerate(feature_names[:10]):\n",
        "            print(f\"        {feature_name}\")\n",
        "\n",
        "    # Route-specific features\n",
        "    if 'ROUTENAME' in df.columns:\n",
        "        route_mapping = {\n",
        "            'ORAL': ['ORAL', 'BY MOUTH', 'ENTERAL'],\n",
        "            'TOPICAL': ['TOPICAL', 'CUTANEOUS', 'TRANSDERMAL', 'SKIN'],\n",
        "            'INJECTION': ['INJECTION', 'INTRAVENOUS', 'INTRAMUSCULAR', 'SUBCUTANEOUS'],\n",
        "            'OPHTHALMIC': ['OPHTHALMIC', 'EYE'],\n",
        "            'NASAL': ['NASAL', 'INTRANASAL'],\n",
        "            'RECTAL': ['RECTAL'],\n",
        "            'VAGINAL': ['VAGINAL'],\n",
        "            'INHALATION': ['INHALATION', 'RESPIRATORY (INHALATION)']\n",
        "        }\n",
        "        for route_category, route_terms in route_mapping.items():\n",
        "            pattern = '|'.join(route_terms)\n",
        "            df[f'IS_ROUTE_{route_category}'] = df['ROUTENAME'].str.contains(pattern, case=False, na=False).astype(np.int8)\n",
        "\n",
        "    # Memory optimization\n",
        "    int_columns = [col for col in df.columns if '_COUNT' in col or 'IS_' in col]\n",
        "    for col in int_columns:\n",
        "        df[col] = df[col].fillna(0).astype(np.int8)\n",
        "\n",
        "    float_columns = [col for col in df.columns if '_LENGTH' in col or 'TFIDF' in col]\n",
        "    for col in float_columns:\n",
        "        df[col] = df[col].astype(np.float16)\n",
        "\n",
        "    gc.collect()\n",
        "    print(\"Feature engineering completed\")\n",
        "    print(f\"Memory usage after engineering: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "GVUjgiwRRXK9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Prepration for Model training and testing"
      ],
      "metadata": {
        "id": "QsUyOGPBDhrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_for_modeling(df):\n",
        "    \"\"\"\n",
        "    Prepare the data for modeling with memory optimization\n",
        "    \"\"\"\n",
        "    print(\"Preparing data for modeling...\")\n",
        "\n",
        "    # 5.1 Select important features\n",
        "    # First, get all available columns by type\n",
        "    text_cols = ['PROPRIETARYNAME', 'DOSAGEFORMNAME', 'ROUTENAME', 'LABELERNAME']\n",
        "    length_cols = [col for col in df.columns if '_LENGTH' in col]\n",
        "    count_cols = [col for col in df.columns if '_COUNT' in col]\n",
        "    binary_cols = [col for col in df.columns if 'IS_' in col]\n",
        "    tfidf_cols = [col for col in df.columns if 'TFIDF' in col]\n",
        "    other_cols = ['MARKETING_YEAR']\n",
        "\n",
        "    # Combine all feature columns\n",
        "    all_feature_cols = text_cols + length_cols + count_cols + binary_cols + tfidf_cols + other_cols\n",
        "\n",
        "    # Filter to only include columns that actually exist in the dataframe\n",
        "    feature_cols = [col for col in all_feature_cols if col in df.columns]\n",
        "\n",
        "    # 5.2 Define features and target\n",
        "    y = df['GROUPED_MARKETING_CATEGORY']\n",
        "    X = df[feature_cols]\n",
        "\n",
        "    # 5.3 Split data with stratification\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    # Log class distribution\n",
        "    print(\"Class distribution in training set:\")\n",
        "    print(y_train.value_counts(normalize=True))\n",
        "\n",
        "    print(\"Class distribution in test set:\")\n",
        "    print(y_test.value_counts(normalize=True))\n",
        "\n",
        "    # Free up memory\n",
        "    del X, y\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"Train set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
        "    print(f\"Test set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
        "    return X_train, X_test, y_train, y_test, feature_cols"
      ],
      "metadata": {
        "id": "qDSKyXtIR8LK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model Traning - Random Forest, Logistic Regression and XG-Boost\n",
        "\n",
        "Logistic Regression is run to show linear model is not suitable for this methods\n",
        "\n"
      ],
      "metadata": {
        "id": "h3KGugRHDxba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_models(X_train, X_test, y_train, y_test, feature_cols):\n",
        "    \"\"\"\n",
        "    Train and evaluate multiple machine learning models\n",
        "    \"\"\"\n",
        "    print(\"Starting model training and evaluation...\")\n",
        "\n",
        "    # Encode target variable if it's categorical\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "    y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "    # Store the mapping for later reference\n",
        "    class_mapping = dict(zip(range(len(label_encoder.classes_)), label_encoder.classes_))\n",
        "    print(\"Class mapping:\", class_mapping)\n",
        "\n",
        "    # Save the label encoder for future use\n",
        "    joblib.dump(label_encoder, 'models/label_encoder.pkl')\n",
        "\n",
        "    # Create pipelines for text and numeric features\n",
        "    # For text features, we'll use a separate pipeline\n",
        "    text_cols = ['PROPRIETARYNAME', 'DOSAGEFORMNAME', 'ROUTENAME', 'LABELERNAME']\n",
        "    text_cols = [col for col in text_cols if col in X_train.columns]\n",
        "\n",
        "    # For numeric features, we'll use a simple imputer\n",
        "    numeric_cols = [col for col in X_train.columns if col not in text_cols]\n",
        "\n",
        "    # Random Forest Model\n",
        "    print(\"\\nTraining Random Forest Classifier...\")\n",
        "    rf_model = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=15,\n",
        "        min_samples_split=10,\n",
        "        min_samples_leaf=5,\n",
        "        random_state=42,\n",
        "        n_jobs=-1  # Use all available cores\n",
        "    )\n",
        "\n",
        "    # Encode all text columns before training\n",
        "    text_cols = ['PROPRIETARYNAME', 'DOSAGEFORMNAME', 'ROUTENAME', 'LABELERNAME']\n",
        "    text_cols = [col for col in text_cols if col in X_train.columns]\n",
        "\n",
        "    for col in text_cols:\n",
        "      le = LabelEncoder()\n",
        "      all_values = pd.concat([X_train[col], X_test[col]]).astype(str)\n",
        "      le.fit(all_values)\n",
        "\n",
        "      X_train[col] = le.transform(X_train[col].astype(str))\n",
        "      X_test[col] = le.transform(X_test[col].astype(str))\n",
        "\n",
        "      # Optional: Save the encoders if needed later\n",
        "      joblib.dump(le, f'models/{col}_encoder_rf.pkl')\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    rf_pred = rf_model.predict(X_test)\n",
        "    rf_pred_proba = rf_model.predict_proba(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"\\nRandom Forest Classification Report:\")\n",
        "    print(classification_report(y_test, rf_pred))\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm = confusion_matrix(y_test, rf_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=label_encoder.classes_,\n",
        "                yticklabels=label_encoder.classes_)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Random Forest Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/rf_confusion_matrix.jpg')\n",
        "    plt.close()\n",
        "\n",
        "    # Feature importance\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    importances = rf_model.feature_importances_\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "    plt.title('Random Forest Feature Importances')\n",
        "    plt.bar(range(min(20, len(importances))),\n",
        "            importances[indices][:20],\n",
        "            color='royalblue',\n",
        "            align='center')\n",
        "    plt.xticks(range(min(20, len(importances))),\n",
        "               np.array(X_train.columns)[indices][:20],\n",
        "               rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/rf_feature_importance.jpg')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot ROC curves\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    n_classes = len(label_encoder.classes_)\n",
        "\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(\n",
        "            (y_test_encoded == i).astype(int),\n",
        "            rf_pred_proba[:, i]\n",
        "        )\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        plt.plot(fpr[i], tpr[i], lw=2,\n",
        "                 label=f'ROC curve for {class_mapping[i]} (area = {roc_auc[i]:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Random Forest: ROC Curves per Class')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/random_forest_roc_curves.jpg')\n",
        "    plt.close()\n",
        "\n",
        "    # Save the model\n",
        "    joblib.dump(rf_model, 'models/random_forest_model.pkl')\n",
        "    print(\"Random Forest model saved to models/random_forest_model.pkl\")\n",
        "\n",
        "    # Release memory\n",
        "    del rf_model, rf_pred, rf_pred_proba\n",
        "    gc.collect()\n",
        "\n",
        "    imputer = SimpleImputer(strategy='mean')  # or 'most_frequent' for categorical\n",
        "\n",
        "    X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
        "    X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
        "\n",
        "    # Logistic Regression Model\n",
        "    print(\"\\nTraining Logistic Regression Classifier...\")\n",
        "    lr_model = LogisticRegression(\n",
        "        C=1.0,\n",
        "        max_iter=1000,\n",
        "        multi_class='multinomial',\n",
        "        solver='saga',\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    #lr_model.fit(X_train, y_train)\n",
        "    lr_model.fit(X_train_imputed, y_train)\n",
        "\n",
        "    # Making predictions\n",
        "    # lr_pred = lr_model.predict(X_test)\n",
        "    # lr_pred_proba = lr_model.predict_proba(X_test)\n",
        "    lr_pred = lr_model.predict(X_test_imputed)\n",
        "    lr_pred_proba = lr_model.predict_proba(X_test_imputed)\n",
        "\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"\\nLogistic Regression Classification Report:\")\n",
        "    print(classification_report(y_test, lr_pred))\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm = confusion_matrix(y_test, lr_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=label_encoder.classes_,\n",
        "                yticklabels=label_encoder.classes_)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Logistic Regression Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/lr_confusion_matrix.jpg')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot ROC curves\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(\n",
        "            (y_test_encoded == i).astype(int),\n",
        "            lr_pred_proba[:, i]\n",
        "        )\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        plt.plot(fpr[i], tpr[i], lw=2,\n",
        "                 label=f'ROC curve for {class_mapping[i]} (area = {roc_auc[i]:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Logistic Regression: ROC Curves per Class')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/logistic_regression_roc_curves.jpg')\n",
        "    plt.close()\n",
        "\n",
        "    # Save the model\n",
        "    joblib.dump(lr_model, 'models/logistic_regression_model.pkl')\n",
        "    print(\"Logistic Regression model saved to models/logistic_regression_model.pkl\")\n",
        "\n",
        "    # Release memory\n",
        "    del lr_model, lr_pred, lr_pred_proba\n",
        "    gc.collect()\n",
        "\n",
        "    # XGBoost Model\n",
        "    print(\"\\nTraining XGBoost Classifier...\")\n",
        "    xgb_model = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='mlogloss'\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    xgb_model.fit(X_train, y_train_encoded)\n",
        "\n",
        "    # Make predictions\n",
        "    xgb_pred = xgb_model.predict(X_test)\n",
        "    xgb_pred_class = label_encoder.inverse_transform(xgb_pred)\n",
        "    xgb_pred_proba = xgb_model.predict_proba(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"\\nXGBoost Classification Report:\")\n",
        "    print(classification_report(y_test, xgb_pred_class))\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm = confusion_matrix(y_test, xgb_pred_class)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=label_encoder.classes_,\n",
        "                yticklabels=label_encoder.classes_)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('XGBoost Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/xgb_confusion_matrix.jpg')\n",
        "    plt.close()\n",
        "\n",
        "    # Feature importance\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    xgb_importance = xgb_model.feature_importances_\n",
        "    indices = np.argsort(xgb_importance)[::-1]\n",
        "    plt.title('XGBoost Feature Importances')\n",
        "    plt.bar(range(min(20, len(xgb_importance))),\n",
        "            xgb_importance[indices][:20],\n",
        "            color='forestgreen',\n",
        "            align='center')\n",
        "    plt.xticks(range(min(20, len(xgb_importance))),\n",
        "               np.array(X_train.columns)[indices][:20],\n",
        "               rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/xgb_feature_importance.jpg')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot ROC curves\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(\n",
        "            (y_test_encoded == i).astype(int),\n",
        "            xgb_pred_proba[:, i]\n",
        "        )\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        plt.plot(fpr[i], tpr[i], lw=2,\n",
        "                 label=f'ROC curve for {class_mapping[i]} (area = {roc_auc[i]:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('XGBoost: ROC Curves per Class')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/xgboost_roc_curves.jpg')\n",
        "    plt.close()\n",
        "\n",
        "    # Save the model\n",
        "    joblib.dump(xgb_model, 'models/xgboost_model.pkl')\n",
        "    print(\"XGBoost model saved to models/xgboost_model.pkl\")\n",
        "\n",
        "    # Exporting test predictions for all models to csv\n",
        "    print(\"\\nExporting predictions to CSV...\")\n",
        "\n",
        "    # Reload models to ensure we're using the saved versions\n",
        "    rf_model = joblib.load('models/random_forest_model.pkl')\n",
        "    lr_model = joblib.load('models/logistic_regression_model.pkl')\n",
        "    xgb_model = joblib.load('models/xgboost_model.pkl')\n",
        "\n",
        "    # Generate predictions\n",
        "    rf_preds = rf_model.predict(X_test)\n",
        "    # lr_preds = lr_model.predict(X_test)\n",
        "    lr_preds = lr_model.predict(X_test_imputed)\n",
        "    xgb_preds = label_encoder.inverse_transform(xgb_model.predict(X_test))\n",
        "\n",
        "    # Create a DataFrame with predictions\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'true_label': y_test.values,\n",
        "        'rf_prediction': rf_preds,\n",
        "        'lr_prediction': lr_preds,\n",
        "        'xgb_prediction': xgb_preds\n",
        "    })\n",
        "\n",
        "    # Save to CSV\n",
        "    predictions_df.to_csv('outputs/test_predictions.csv', index=False)\n",
        "    print(\"Predictions saved to outputs/test_predictions.csv\")\n",
        "\n",
        "    # SHAP values for model interpretability (for Random Forest)\n",
        "    print(\"\\nCalculating SHAP values for model interpretability...\")\n",
        "    try:\n",
        "        # Create a smaller sample for SHAP analysis to avoid memory issues\n",
        "        sample_size = min(1000, X_test.shape[0])\n",
        "        X_sample = X_test.sample(sample_size, random_state=42)\n",
        "\n",
        "        # Calculate SHAP values for Random Forest\n",
        "        explainer = shap.TreeExplainer(rf_model)\n",
        "        shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "        # Plot summary\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False)\n",
        "        plt.title('SHAP Feature Importance')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('outputs/shap_feature_importance.jpg')\n",
        "        plt.close()\n",
        "\n",
        "        # Plot detailed SHAP values for top class\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        shap.summary_plot(shap_values[0], X_sample, show=False)\n",
        "        plt.title(f'SHAP Values for Class: {label_encoder.classes_[0]}')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('outputs/shap_values_detail.jpg')\n",
        "        plt.close()\n",
        "\n",
        "        print(\"SHAP analysis complete and visualizations saved.\")\n",
        "    except Exception as e:\n",
        "        print(f\"SHAP analysis error: {e}\")\n",
        "        print(\"Skipping SHAP analysis due to error.\")\n",
        "\n",
        "    # Release memory\n",
        "    del rf_model, lr_model, xgb_model\n",
        "    gc.collect()\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "thfv_RUFhQ5W"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Leanring Model - Multi Layer Perceptron(MLP) Neural Network"
      ],
      "metadata": {
        "id": "3xM2oW2tEqTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_deep_learning_model(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Train and evaluate a deep learning model\n",
        "    \"\"\"\n",
        "    print(\"\\nTraining Deep Learning Model...\")\n",
        "\n",
        "    # Check if GPU is available\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        print(\"GPU is available. Using GPU for training.\")\n",
        "    else:\n",
        "        print(\"GPU is not available. Using CPU for training.\")\n",
        "\n",
        "    # Encode target variable if it's categorical\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "    y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "    # Convert to one-hot encoding\n",
        "    n_classes = len(label_encoder.classes_)\n",
        "    y_train_onehot = to_categorical(y_train_encoded, num_classes=n_classes)\n",
        "    y_test_onehot = to_categorical(y_test_encoded, num_classes=n_classes)\n",
        "\n",
        "    # Print class mapping\n",
        "    class_mapping = dict(zip(range(len(label_encoder.classes_)), label_encoder.classes_))\n",
        "    print(\"Class mapping:\", class_mapping)\n",
        "\n",
        "    # Separate features by type\n",
        "    # Text features\n",
        "    text_cols = ['PROPRIETARYNAME', 'DOSAGEFORMNAME', 'ROUTENAME', 'LABELERNAME']\n",
        "    text_cols = [col for col in text_cols if col in X_train.columns]\n",
        "\n",
        "    # Numeric features\n",
        "    numeric_cols = [col for col in X_train.columns if col not in text_cols]\n",
        "\n",
        "    # Fill NaNs in numeric features\n",
        "    X_train[numeric_cols] = X_train[numeric_cols].fillna(0)\n",
        "    X_test[numeric_cols] = X_test[numeric_cols].fillna(0)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X_train_numeric = X_train[numeric_cols].values\n",
        "    X_test_numeric = X_test[numeric_cols].values\n",
        "\n",
        "    # Normalize numeric features\n",
        "    numeric_mean = X_train_numeric.mean(axis=0)\n",
        "    numeric_std = X_train_numeric.std(axis=0)\n",
        "    numeric_std[numeric_std == 0] = 1  # Avoid division by zero\n",
        "    X_train_numeric = (X_train_numeric - numeric_mean) / numeric_std\n",
        "    X_test_numeric = (X_test_numeric - numeric_mean) / numeric_std\n",
        "\n",
        "    # Save normalizer parameters\n",
        "    np.save('models/numeric_mean.npy', numeric_mean)\n",
        "    np.save('models/numeric_std.npy', numeric_std)\n",
        "\n",
        "    # Building model\n",
        "    print(\"Building neural network model...\")\n",
        "\n",
        "    # Input layer for numeric features\n",
        "    numeric_input = Input(shape=(X_train_numeric.shape[1],), name='numeric_input')\n",
        "\n",
        "    # First hidden layer with batch normalization\n",
        "    x = Dense(128, activation='relu')(numeric_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    # Second hidden layer\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # Output layer\n",
        "    output = Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    # Compile model\n",
        "    model = Model(inputs=numeric_input, outputs=output)\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Print model summary\n",
        "    model.summary()\n",
        "\n",
        "    # Define callbacks\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=3,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nTraining neural network...\")\n",
        "    history = model.fit(\n",
        "        X_train_numeric,\n",
        "        y_train_onehot,\n",
        "        epochs=20,\n",
        "        batch_size=256,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"\\nEvaluating neural network...\")\n",
        "    test_loss, test_acc = model.evaluate(X_test_numeric, y_test_onehot, verbose=0)\n",
        "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Test loss: {test_loss:.4f}\")\n",
        "\n",
        "    # Making predictions\n",
        "    dl_pred_proba = model.predict(X_test_numeric)\n",
        "    dl_pred_class = np.argmax(dl_pred_proba, axis=1)\n",
        "    dl_pred = label_encoder.inverse_transform(dl_pred_class)\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/dl_training_history.jpg')\n",
        "    plt.close()\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"\\nDeep Learning Classification Report:\")\n",
        "    print(classification_report(y_test, dl_pred))\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm = confusion_matrix(y_test, dl_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=label_encoder.classes_,\n",
        "                yticklabels=label_encoder.classes_)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Deep Learning Model Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/dl_confusion_matrix.jpg')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot ROC curves\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(\n",
        "            (y_test_encoded == i).astype(int),\n",
        "            dl_pred_proba[:, i]\n",
        "        )\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        plt.plot(fpr[i], tpr[i], lw=2,\n",
        "                 label=f'ROC curve for {class_mapping[i]} (area = {roc_auc[i]:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Deep Learning: ROC Curves per Class')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/deep_learning_roc_curves.jpg')\n",
        "    plt.close()\n",
        "\n",
        "    # Saving the model\n",
        "    model.save('models/dl_model.keras')\n",
        "    print(\"Deep Learning model saved to models/dl_model.keras\")\n",
        "\n",
        "    # Release memory\n",
        "    del model, history\n",
        "    gc.collect()\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "qBoBeNenhRyp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improving Deep Learning Model with Embeddings"
      ],
      "metadata": {
        "id": "VfoYUU9EFJmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_deep_learning_model_with_embeddings(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Train and evaluate an improved deep learning model with embeddings for categorical features\n",
        "    \"\"\"\n",
        "    print(\"\\nTraining Improved Deep Learning Model with Embeddings...\")\n",
        "\n",
        "    # Checking if GPU is available\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        print(\"GPU is available. Using GPU for training.\")\n",
        "    else:\n",
        "        print(\"GPU is not available. Using CPU for training.\")\n",
        "\n",
        "    # Encode target variable if it's categorical\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "    y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "    # Convert to one-hot encoding\n",
        "    n_classes = len(label_encoder.classes_)\n",
        "    y_train_onehot = to_categorical(y_train_encoded, num_classes=n_classes)\n",
        "    y_test_onehot = to_categorical(y_test_encoded, num_classes=n_classes)\n",
        "\n",
        "    # Print class mapping\n",
        "    class_mapping = dict(zip(range(len(label_encoder.classes_)), label_encoder.classes_))\n",
        "    print(\"Class mapping:\", class_mapping)\n",
        "\n",
        "    # Separating features by type\n",
        "    # Text features to use for embeddings\n",
        "    text_cols = ['PROPRIETARYNAME', 'DOSAGEFORMNAME', 'ROUTENAME', 'LABELERNAME']\n",
        "    text_cols = [col for col in text_cols if col in X_train.columns]\n",
        "\n",
        "    # Numeric features\n",
        "    numeric_cols = [col for col in X_train.columns if col not in text_cols]\n",
        "\n",
        "    X_train[numeric_cols] = X_train[numeric_cols].fillna(0)\n",
        "    X_test[numeric_cols] = X_test[numeric_cols].fillna(0)\n",
        "\n",
        "    # Convert numeric data to numpy arrays\n",
        "    X_train_numeric = X_train[numeric_cols].values\n",
        "    X_test_numeric = X_test[numeric_cols].values\n",
        "\n",
        "    # Normalize numeric features\n",
        "    numeric_mean = X_train_numeric.mean(axis=0)\n",
        "    numeric_std = X_train_numeric.std(axis=0)\n",
        "    numeric_std[numeric_std == 0] = 1  # Avoid division by zero\n",
        "    X_train_numeric = (X_train_numeric - numeric_mean) / numeric_std\n",
        "    X_test_numeric = (X_test_numeric - numeric_mean) / numeric_std\n",
        "\n",
        "    # Save normalizer parameters\n",
        "    np.save('models/dl_improved_numeric_mean.npy', numeric_mean)\n",
        "    np.save('models/dl_improved_numeric_std.npy', numeric_std)\n",
        "\n",
        "    # Preparing text data for embeddings\n",
        "    print(\"Processing text features for embeddings...\")\n",
        "    # Dictionary to store label encoders for each text column\n",
        "    text_encoders = {}\n",
        "\n",
        "    # Process each text column\n",
        "    for col in text_cols:\n",
        "        print(f\"Encoding text column: {col}\")\n",
        "        # Create and fit a label encoder for this column\n",
        "        col_encoder = LabelEncoder()\n",
        "        # Combine train and test values to ensure all categories are represented\n",
        "        all_values = pd.concat([X_train[col], X_test[col]]).unique()\n",
        "        col_encoder.fit(all_values)\n",
        "\n",
        "        # Transform train and test data\n",
        "        X_train[f\"{col}_encoded\"] = col_encoder.transform(X_train[col])\n",
        "        X_test[f\"{col}_encoded\"] = col_encoder.transform(X_test[col])\n",
        "\n",
        "        # Store the encoder\n",
        "        text_encoders[col] = col_encoder\n",
        "\n",
        "        # Save the encoder\n",
        "        joblib.dump(col_encoder, f'models/{col}_encoder.pkl')\n",
        "        print(f\"  - {col} has {len(col_encoder.classes_)} unique values\")\n",
        "\n",
        "    # Building model with embeddings\n",
        "    print(\"Building neural network model with embeddings...\")\n",
        "\n",
        "    # Input layer for numeric features\n",
        "    numeric_input = Input(shape=(X_train_numeric.shape[1],), name='numeric_input')\n",
        "    numeric_dense = Dense(64, activation='relu')(numeric_input)\n",
        "    numeric_bn = BatchNormalization()(numeric_dense)\n",
        "\n",
        "    # Input layers and embedding layers for text features\n",
        "    text_inputs = []\n",
        "    text_embeddings = []\n",
        "\n",
        "    for col in text_cols:\n",
        "        # Get vocabulary size\n",
        "        vocab_size = len(text_encoders[col].classes_)\n",
        "\n",
        "        # Create input layer\n",
        "        text_input = Input(shape=(1,), name=f'{col}_input')\n",
        "        text_inputs.append(text_input)\n",
        "\n",
        "        # Create embedding layer (embedding size depends on vocabulary size)\n",
        "        embedding_size = min(50, (vocab_size + 1) // 2)  # Rule of thumb\n",
        "        embedding = Embedding(input_dim=vocab_size, output_dim=embedding_size,\n",
        "                             name=f'{col}_embedding')(text_input)\n",
        "        embedding = Flatten(name=f'{col}_flatten')(embedding)\n",
        "        text_embeddings.append(embedding)\n",
        "\n",
        "    # Concatenate all inputs\n",
        "    if text_embeddings:\n",
        "        concatenated = Concatenate()([numeric_bn] + text_embeddings)\n",
        "    else:\n",
        "        concatenated = numeric_bn\n",
        "\n",
        "    # Hidden layers\n",
        "    x = Dense(128, activation='relu')(concatenated)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # Output layer\n",
        "    output = Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    # Compile model\n",
        "    if text_inputs:\n",
        "        model = Model(inputs=[numeric_input] + text_inputs, outputs=output)\n",
        "    else:\n",
        "        model = Model(inputs=numeric_input, outputs=output)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Print model summary\n",
        "    model.summary()\n",
        "\n",
        "    # Define callbacks to stop the model by overfitting\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=5,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Prepare inputs for training\n",
        "    train_inputs = [X_train_numeric]\n",
        "    test_inputs = [X_test_numeric]\n",
        "\n",
        "    for col in text_cols:\n",
        "        train_inputs.append(X_train[f\"{col}_encoded\"].values.reshape(-1, 1))\n",
        "        test_inputs.append(X_test[f\"{col}_encoded\"].values.reshape(-1, 1))\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nTraining improved neural network with embeddings...\")\n",
        "    history = model.fit(\n",
        "        train_inputs,\n",
        "        y_train_onehot,\n",
        "        epochs=30,\n",
        "        batch_size=256,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"\\nEvaluating improved neural network...\")\n",
        "    test_loss, test_acc = model.evaluate(test_inputs, y_test_onehot, verbose=0)\n",
        "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Test loss: {test_loss:.4f}\")\n",
        "\n",
        "    # Make predictions\n",
        "    dl_pred_proba = model.predict(test_inputs)\n",
        "    dl_pred_class = np.argmax(dl_pred_proba, axis=1)\n",
        "    dl_pred = label_encoder.inverse_transform(dl_pred_class)\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/dl_improved_training_history.jpg')\n",
        "    plt.close()\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"\\nImproved Deep Learning Classification Report:\")\n",
        "    print(classification_report(y_test, dl_pred))\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm = confusion_matrix(y_test, dl_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "              xticklabels=label_encoder.classes_,\n",
        "              yticklabels=label_encoder.classes_)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Improved Deep Learning Model Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/dl_improved_confusion_matrix.jpg')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot ROC curves\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(\n",
        "            (y_test_encoded == i).astype(int),\n",
        "            dl_pred_proba[:, i]\n",
        "        )\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        plt.plot(fpr[i], tpr[i], lw=2,\n",
        "               label=f'ROC curve for {class_mapping[i]} (area = {roc_auc[i]:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Improved Deep Learning: ROC Curves per Class')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/deep_learning_improved_roc_curves.jpg')\n",
        "    plt.close()\n",
        "\n",
        "    # Save the model\n",
        "    model.save('models/dl_improved_model.keras')\n",
        "    print(\"Improved Deep Learning model saved to models/dl_improved_model.keras\")\n",
        "\n",
        "    # Release memory\n",
        "    del model, history\n",
        "    gc.collect()\n",
        "    return"
      ],
      "metadata": {
        "id": "tbyXxhd_hV0s"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution Fuction"
      ],
      "metadata": {
        "id": "mZq1JvoSF2b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(data_path):\n",
        "    \"\"\"\n",
        "    Main function to orchestrate the entire pipeline\n",
        "    \"\"\"\n",
        "    print(\"Starting FDA Drug Product Classification pipeline...\")\n",
        "\n",
        "    # Load data\n",
        "    df = load_data(data_path)\n",
        "    if df is None:\n",
        "        print(\"Failed to load data. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Clean and preprocess data\n",
        "    df = clean_and_preprocess_data(df)\n",
        "\n",
        "    # Engineer features\n",
        "    df = engineer_features(df)\n",
        "\n",
        "    # Prepare data for modeling\n",
        "    X_train, X_test, y_train, y_test, feature_cols = prepare_data_for_modeling(df)\n",
        "\n",
        "    # Train and evaluate machine learning models\n",
        "    train_and_evaluate_models(X_train, X_test, y_train, y_test, feature_cols)\n",
        "\n",
        "    # Train and evaluate basic deep learning model\n",
        "    train_deep_learning_model(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Train and evaluate improved deep learning model with embeddings\n",
        "    train_deep_learning_model_with_embeddings(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Print final summary\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FDA Drug Product Classification Pipeline Complete\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"\\nOutput files are saved in the 'outputs' directory.\")\n",
        "    print(\"Trained models are saved in the 'models' directory.\")\n",
        "    print(\"\\nSummary of models:\")\n",
        "    print(\"1. Random Forest\")\n",
        "    print(\"2. Logistic Regression\")\n",
        "    print(\"3. XGBoost\")\n",
        "    print(\"4. Basic Deep Learning Model\")\n",
        "    print(\"5. Improved Deep Learning Model with Embeddings\")\n",
        "    print(\"\\nSee classification reports and visualizations for performance metrics.\")\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "sSNaJOG1keR_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Specify the path to your data\n",
        "    # data_path = \"/content/drive/MyDrive/ML_Project/fda_product.xlsx\"\n",
        "    data_path = \"/content/fda_product.xlsx\"\n",
        "\n",
        "    # Run the main function\n",
        "    main(data_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "604s27swkjEk",
        "outputId": "c23c2397-a351-4ebd-d3d2-4dc1e4c13b0a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting FDA Drug Product Classification pipeline...\n",
            "Loading data from /content/fda_product.xlsx...\n",
            "Data loaded successfully with 111970 rows and 9 columns.\n",
            "Memory usage: 7.69 MB\n",
            "Starting data cleaning and preprocessing...\n",
            "Data cleaned and preprocessed. Remaining rows: 111962\n",
            "Starting feature engineering...\n",
            "Generating TF-IDF features for SUBSTANCENAME...\n",
            "Top TF-IDF terms:\n",
            "        acetaminophen\n",
            "        acetate\n",
            "        acid\n",
            "        alcohol\n",
            "        aluminum\n",
            "        arsenic\n",
            "        avobenzone\n",
            "        bark\n",
            "        benzalkonium\n",
            "        calcium\n",
            "Feature engineering completed\n",
            "Memory usage after engineering: 22.21 MB\n",
            "Preparing data for modeling...\n",
            "Class distribution in training set:\n",
            "GROUPED_MARKETING_CATEGORY\n",
            "ANDA          0.434883\n",
            "OTC           0.302080\n",
            "UNAPPROVED    0.146948\n",
            "NDA           0.079737\n",
            "BLA           0.036352\n",
            "Name: proportion, dtype: float64\n",
            "Class distribution in test set:\n",
            "GROUPED_MARKETING_CATEGORY\n",
            "ANDA          0.434868\n",
            "OTC           0.302103\n",
            "UNAPPROVED    0.146966\n",
            "NDA           0.079712\n",
            "BLA           0.036351\n",
            "Name: proportion, dtype: float64\n",
            "Train set: 89569 samples, 82 features\n",
            "Test set: 22393 samples, 82 features\n",
            "Starting model training and evaluation...\n",
            "Class mapping: {0: 'ANDA', 1: 'BLA', 2: 'NDA', 3: 'OTC', 4: 'UNAPPROVED'}\n",
            "\n",
            "Training Random Forest Classifier...\n",
            "\n",
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ANDA       0.88      0.98      0.93      9738\n",
            "         BLA       0.92      0.91      0.92       814\n",
            "         NDA       0.84      0.40      0.54      1785\n",
            "         OTC       0.93      0.95      0.94      6765\n",
            "  UNAPPROVED       0.97      0.88      0.92      3291\n",
            "\n",
            "    accuracy                           0.91     22393\n",
            "   macro avg       0.91      0.82      0.85     22393\n",
            "weighted avg       0.91      0.91      0.90     22393\n",
            "\n",
            "Random Forest model saved to models/random_forest_model.pkl\n",
            "\n",
            "Training Logistic Regression Classifier...\n",
            "\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ANDA       0.58      0.87      0.69      9738\n",
            "         BLA       0.00      0.00      0.00       814\n",
            "         NDA       0.00      0.00      0.00      1785\n",
            "         OTC       0.66      0.64      0.65      6765\n",
            "  UNAPPROVED       0.74      0.25      0.38      3291\n",
            "\n",
            "    accuracy                           0.61     22393\n",
            "   macro avg       0.39      0.35      0.34     22393\n",
            "weighted avg       0.56      0.61      0.55     22393\n",
            "\n",
            "Logistic Regression model saved to models/logistic_regression_model.pkl\n",
            "\n",
            "Training XGBoost Classifier...\n",
            "\n",
            "XGBoost Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ANDA       0.91      0.97      0.94      9738\n",
            "         BLA       0.92      0.94      0.93       814\n",
            "         NDA       0.82      0.55      0.66      1785\n",
            "         OTC       0.94      0.96      0.95      6765\n",
            "  UNAPPROVED       0.96      0.90      0.93      3291\n",
            "\n",
            "    accuracy                           0.92     22393\n",
            "   macro avg       0.91      0.86      0.88     22393\n",
            "weighted avg       0.92      0.92      0.92     22393\n",
            "\n",
            "XGBoost model saved to models/xgboost_model.pkl\n",
            "\n",
            "Exporting predictions to CSV...\n",
            "Predictions saved to outputs/test_predictions.csv\n",
            "\n",
            "Calculating SHAP values for model interpretability...\n",
            "SHAP analysis error: The shape of the shap_values matrix does not match the shape of the provided data matrix.\n",
            "Skipping SHAP analysis due to error.\n",
            "\n",
            "Training Deep Learning Model...\n",
            "GPU is not available. Using CPU for training.\n",
            "Class mapping: {0: 'ANDA', 1: 'BLA', 2: 'NDA', 3: 'OTC', 4: 'UNAPPROVED'}\n",
            "Building neural network model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ numeric_input (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m78\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m10,112\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m325\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ numeric_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">78</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,112</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,461\u001b[0m (76.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,461</span> (76.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,077\u001b[0m (74.52 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,077</span> (74.52 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training neural network...\n",
            "Epoch 1/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.6664 - loss: 0.9852 - val_accuracy: 0.8321 - val_loss: 0.4789 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8146 - loss: 0.5413 - val_accuracy: 0.8436 - val_loss: 0.4269 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8259 - loss: 0.4915 - val_accuracy: 0.8486 - val_loss: 0.4033 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8352 - loss: 0.4617 - val_accuracy: 0.8604 - val_loss: 0.3896 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8443 - loss: 0.4416 - val_accuracy: 0.8627 - val_loss: 0.3758 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.8480 - loss: 0.4262 - val_accuracy: 0.8691 - val_loss: 0.3665 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8543 - loss: 0.4121 - val_accuracy: 0.8695 - val_loss: 0.3579 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8581 - loss: 0.4076 - val_accuracy: 0.8751 - val_loss: 0.3517 - learning_rate: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8644 - loss: 0.3882 - val_accuracy: 0.8805 - val_loss: 0.3449 - learning_rate: 0.0010\n",
            "Epoch 10/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8681 - loss: 0.3820 - val_accuracy: 0.8817 - val_loss: 0.3402 - learning_rate: 0.0010\n",
            "Epoch 11/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8679 - loss: 0.3771 - val_accuracy: 0.8820 - val_loss: 0.3350 - learning_rate: 0.0010\n",
            "Epoch 12/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8695 - loss: 0.3746 - val_accuracy: 0.8855 - val_loss: 0.3330 - learning_rate: 0.0010\n",
            "Epoch 13/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8691 - loss: 0.3726 - val_accuracy: 0.8827 - val_loss: 0.3319 - learning_rate: 0.0010\n",
            "Epoch 14/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8758 - loss: 0.3621 - val_accuracy: 0.8887 - val_loss: 0.3252 - learning_rate: 0.0010\n",
            "Epoch 15/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8760 - loss: 0.3582 - val_accuracy: 0.8914 - val_loss: 0.3241 - learning_rate: 0.0010\n",
            "Epoch 16/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8768 - loss: 0.3575 - val_accuracy: 0.8909 - val_loss: 0.3228 - learning_rate: 0.0010\n",
            "Epoch 17/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8769 - loss: 0.3582 - val_accuracy: 0.8922 - val_loss: 0.3174 - learning_rate: 0.0010\n",
            "Epoch 18/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8761 - loss: 0.3593 - val_accuracy: 0.8898 - val_loss: 0.3176 - learning_rate: 0.0010\n",
            "Epoch 19/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8817 - loss: 0.3455 - val_accuracy: 0.8934 - val_loss: 0.3124 - learning_rate: 0.0010\n",
            "Epoch 20/20\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8796 - loss: 0.3496 - val_accuracy: 0.8927 - val_loss: 0.3127 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 19.\n",
            "\n",
            "Evaluating neural network...\n",
            "Test accuracy: 0.8888\n",
            "Test loss: 0.3197\n",
            "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Deep Learning Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ANDA       0.88      0.95      0.92      9738\n",
            "         BLA       0.82      0.91      0.86       814\n",
            "         NDA       0.74      0.43      0.54      1785\n",
            "         OTC       0.92      0.95      0.93      6765\n",
            "  UNAPPROVED       0.92      0.83      0.87      3291\n",
            "\n",
            "    accuracy                           0.89     22393\n",
            "   macro avg       0.86      0.81      0.82     22393\n",
            "weighted avg       0.88      0.89      0.88     22393\n",
            "\n",
            "Deep Learning model saved to models/dl_model.keras\n",
            "\n",
            "Training Improved Deep Learning Model with Embeddings...\n",
            "GPU is not available. Using CPU for training.\n",
            "Class mapping: {0: 'ANDA', 1: 'BLA', 2: 'NDA', 3: 'OTC', 4: 'UNAPPROVED'}\n",
            "Processing text features for embeddings...\n",
            "Encoding text column: PROPRIETARYNAME\n",
            "  - PROPRIETARYNAME has 37930 unique values\n",
            "Encoding text column: DOSAGEFORMNAME\n",
            "  - DOSAGEFORMNAME has 139 unique values\n",
            "Encoding text column: ROUTENAME\n",
            "  - ROUTENAME has 204 unique values\n",
            "Encoding text column: LABELERNAME\n",
            "  - LABELERNAME has 7491 unique values\n",
            "Building neural network model with embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ numeric_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m78\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ PROPRIETARYNAME_in… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ DOSAGEFORMNAME_inp… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ROUTENAME_input     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ LABELERNAME_input   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m5,056\u001b[0m │ numeric_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ PROPRIETARYNAME_em… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m50\u001b[0m)     │  \u001b[38;5;34m1,896,500\u001b[0m │ PROPRIETARYNAME_… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ DOSAGEFORMNAME_emb… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m50\u001b[0m)     │      \u001b[38;5;34m6,950\u001b[0m │ DOSAGEFORMNAME_i… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ROUTENAME_embedding │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m50\u001b[0m)     │     \u001b[38;5;34m10,200\u001b[0m │ ROUTENAME_input[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ LABELERNAME_embedd… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m50\u001b[0m)     │    \u001b[38;5;34m374,550\u001b[0m │ LABELERNAME_inpu… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m256\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ PROPRIETARYNAME_fl… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ PROPRIETARYNAME_… │\n",
              "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ DOSAGEFORMNAME_fla… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ DOSAGEFORMNAME_e… │\n",
              "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ROUTENAME_flatten   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ ROUTENAME_embedd… │\n",
              "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ LABELERNAME_flatten │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ LABELERNAME_embe… │\n",
              "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m264\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ PROPRIETARYNAME_… │\n",
              "│                     │                   │            │ DOSAGEFORMNAME_f… │\n",
              "│                     │                   │            │ ROUTENAME_flatte… │\n",
              "│                     │                   │            │ LABELERNAME_flat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m33,920\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m256\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │        \u001b[38;5;34m325\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ numeric_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">78</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ PROPRIETARYNAME_in… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ DOSAGEFORMNAME_inp… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ROUTENAME_input     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ LABELERNAME_input   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,056</span> │ numeric_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ PROPRIETARYNAME_em… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,896,500</span> │ PROPRIETARYNAME_… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ DOSAGEFORMNAME_emb… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,950</span> │ DOSAGEFORMNAME_i… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ROUTENAME_embedding │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,200</span> │ ROUTENAME_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ LABELERNAME_embedd… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">374,550</span> │ LABELERNAME_inpu… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ PROPRIETARYNAME_fl… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ PROPRIETARYNAME_… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ DOSAGEFORMNAME_fla… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ DOSAGEFORMNAME_e… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ROUTENAME_flatten   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ ROUTENAME_embedd… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ LABELERNAME_flatten │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ LABELERNAME_embe… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ PROPRIETARYNAME_… │\n",
              "│                     │                   │            │ DOSAGEFORMNAME_f… │\n",
              "│                     │                   │            │ ROUTENAME_flatte… │\n",
              "│                     │                   │            │ LABELERNAME_flat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,920</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,336,781\u001b[0m (8.91 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,336,781</span> (8.91 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,336,269\u001b[0m (8.91 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,336,269</span> (8.91 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training improved neural network with embeddings...\n",
            "Epoch 1/30\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 30ms/step - accuracy: 0.7343 - loss: 0.7838 - val_accuracy: 0.9387 - val_loss: 0.1896 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - accuracy: 0.9474 - loss: 0.1579 - val_accuracy: 0.9592 - val_loss: 0.1226 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 28ms/step - accuracy: 0.9769 - loss: 0.0734 - val_accuracy: 0.9614 - val_loss: 0.1206 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.9844 - loss: 0.0488 - val_accuracy: 0.9624 - val_loss: 0.1219 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - accuracy: 0.9871 - loss: 0.0411 - val_accuracy: 0.9622 - val_loss: 0.1231 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.9880 - loss: 0.0355 - val_accuracy: 0.9613 - val_loss: 0.1286 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 28ms/step - accuracy: 0.9887 - loss: 0.0328 - val_accuracy: 0.9611 - val_loss: 0.1323 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m278/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9894 - loss: 0.0298\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.9894 - loss: 0.0298 - val_accuracy: 0.9606 - val_loss: 0.1388 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9911 - loss: 0.0258 - val_accuracy: 0.9614 - val_loss: 0.1343 - learning_rate: 2.0000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 28ms/step - accuracy: 0.9924 - loss: 0.0216 - val_accuracy: 0.9614 - val_loss: 0.1341 - learning_rate: 2.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.9923 - loss: 0.0212 - val_accuracy: 0.9621 - val_loss: 0.1370 - learning_rate: 2.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 29ms/step - accuracy: 0.9926 - loss: 0.0214 - val_accuracy: 0.9616 - val_loss: 0.1397 - learning_rate: 2.0000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9924 - loss: 0.0203\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 27ms/step - accuracy: 0.9924 - loss: 0.0203 - val_accuracy: 0.9624 - val_loss: 0.1378 - learning_rate: 2.0000e-04\n",
            "Epoch 13: early stopping\n",
            "Restoring model weights from the end of the best epoch: 3.\n",
            "\n",
            "Evaluating improved neural network...\n",
            "Test accuracy: 0.9603\n",
            "Test loss: 0.1255\n",
            "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
            "\n",
            "Improved Deep Learning Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ANDA       0.96      0.98      0.97      9738\n",
            "         BLA       0.98      0.96      0.97       814\n",
            "         NDA       0.89      0.77      0.83      1785\n",
            "         OTC       0.98      0.98      0.98      6765\n",
            "  UNAPPROVED       0.97      0.96      0.96      3291\n",
            "\n",
            "    accuracy                           0.96     22393\n",
            "   macro avg       0.95      0.93      0.94     22393\n",
            "weighted avg       0.96      0.96      0.96     22393\n",
            "\n",
            "Improved Deep Learning model saved to models/dl_improved_model.keras\n",
            "\n",
            "==================================================\n",
            "FDA Drug Product Classification Pipeline Complete\n",
            "==================================================\n",
            "\n",
            "Output files are saved in the 'outputs' directory.\n",
            "Trained models are saved in the 'models' directory.\n",
            "\n",
            "Summary of models:\n",
            "1. Random Forest\n",
            "2. Logistic Regression\n",
            "3. XGBoost\n",
            "4. Basic Deep Learning Model\n",
            "5. Improved Deep Learning Model with Embeddings\n",
            "\n",
            "See classification reports and visualizations for performance metrics.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}